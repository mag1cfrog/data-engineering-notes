---
title: "First Lesson: Partition vs Broadcast"
pubDate: 2025-06-13T12:00:00-08:00
tags:
  - spark
  - optimization
description: "How switching from partitioned joins to broadcast joins reduced shuffle writes from 8GB to 500MB"
draft: true
---
import { Image } from 'astro:assets';
import queryProfile1 from '../../assets/img/dbx_query_profile_1.jpg';
import queryProfile2 from '../../assets/img/dbx_query_profile_2.jpg';

## Intro

I work for an MLB team, and we process massive amounts of baseball tracking data every day. Recently, we had a Spark job on Databricks that needed to join an enormous table (player tracking records with coordinates for each frame) with a dimensional table (position number to player uid mapping), then perform deduplication using a window function.


### The Deduplication Challenge

The core logic involved a window function to handle duplicate records (some psedudo SQL):

```sql
WITH ranked AS (
  SELECT 
    tracking.*,
    lineup.fielder_id,
    lineup.position_alpha,
    ROW_NUMBER() OVER (
      PARTITION BY game_id, pitch_uid, position_num, event_time
      ORDER BY processed_year DESC, processed_month DESC, processed_day DESC
    ) AS rn
  FROM hawkeye_tracking tracking
  JOIN hawkeye_lineup lineup ON (...)
)
SELECT * EXCEPT (rn)
FROM ranked 
WHERE rn = 1
```

Although the query itself doesn't look complex, the long execution time of it raised some concerns. With the table sized at only a couple hundreds of GBs, why would it take over 30 mins to run on Databricks, even with a medium-sized SQL warehouse? Something's clearly wrong here, so I dove into DBX's query profile to investigate.

## Query Profile Analysis

Looking at the query profile, I immediately spotted the issue:

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 2rem 0;">
  <Image 
    src={queryProfile1} 
    alt="Databricks query profile showing I/O metrics and 33+ execution time" 
    width={500} 
    height={300}
    style="width: 100%; height: 100%; object-fit: contain; border-radius: 8px;"
  />
  <Image 
    src={queryProfile2} 
    alt="Databricks query profile showing massive spill" 
    width={500} 
    height={400}
    style="width: 100%; height: 100%; object-fit: contain; border-radius: 8px;"
  />
</div>

### The Real Problem: Massive Disk Spilling

**391.87 GB of disk spilling**
This was the biggest red flag. The query spilled nearly 400GB to disk, which is almost 3x the amount of data actually read (146.66 GB). This indicates that Spark couldn't fit the intermediate results in memory and had to write them to disk repeatedly - a huge performance killer.
